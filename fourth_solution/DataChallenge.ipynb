{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from sklearn import decomposition\n",
    "import sklearn.feature_extraction.text as text\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/vincent/OneDrive/axa/DS winter school/winter_school_DS_QH2017/prg'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cd /Users/vincent/OneDrive/axa/DS winter school/winter_school_DS_QH2017/prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_string(string):\n",
    "    th_regexp = re.compile('[a-zA-Z\\d]*-th')\n",
    "    citation_regexp = re.compile('[a-zA-Z]+[\\d]+')\n",
    "    digit_regexp = re.compile('[\\d]+')\n",
    "    double_whitespace_regexp = re.compile('\\s[\\s]+')\n",
    "    non_alphabet_regexp = re.compile('[\\W]')\n",
    "    one_letter_regexp = re.compile('\\s[a-zA-Z]\\s')\n",
    "    two_letters_regexp = re.compile('\\s[a-zA-Z][a-zA-Z]\\s')   \n",
    "    \n",
    "    \n",
    "    \n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "    \n",
    "    verbs_tags_set = set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    \n",
    "    resulted_tokens = [token for (token, tag) in tagged_tokens if tag not in verbs_tags_set]\n",
    "    \n",
    "    for (token,tag) in tagged_tokens:\n",
    "        if tag not in verbs_tags_set:\n",
    "            resulted_tokens.append(token)\n",
    "            \n",
    "    result = \" \".join(resulted_tokens)\n",
    "\n",
    "\n",
    "    result = string\n",
    "    result = th_regexp.sub('', result)\n",
    "#     result = citation_regexp.sub('', result)\n",
    "    result = digit_regexp.sub('', result)\n",
    "    result = non_alphabet_regexp.sub(' ', result)\n",
    "    result = one_letter_regexp.sub(' ', result)\n",
    "    result = two_letters_regexp.sub(' ', result)\n",
    "    result = double_whitespace_regexp.sub(' ', result)    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/vincent/OneDrive/axa/DS winter school/winter_school_DS_QH2017/prg'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../input/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "# TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# importance_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.0001, ngram_range=(1,1))\n",
    "# TFIDF_matrix_sparse = importance_vectorizer.fit_transform(corpus)\n",
    "# num_topics = 20\n",
    "# U, S, V = svds(TFIDF_matrix_sparse, k=num_topics)\n",
    "# TOPIC_matrix = np.dot(TFIDF_matrix, V.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [preprocess_string(element[5]) for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "num_topics = 100\n",
    "_,_, V = svds(TFIDF_matrix, k=num_topics)\n",
    "TOPIC_matrix = TFIDF_matrix.dot(V.transpose())\n",
    "del V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "\n",
    "def get_features(data_fname, info_fname, portion, with_labels=True, node_info = None, tfidf_mat=None,\n",
    "                 topic_mat=None, graph=None):\n",
    "    with open(data_fname, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data_set = list(reader)\n",
    "\n",
    "    data_set = [element[0].split(\" \") for element in data_set]\n",
    "    \n",
    "\n",
    "#     with open(info_fname, \"r\") as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         node_info  = list(reader)\n",
    "        \n",
    "        \n",
    "    if portion == 1.0:\n",
    "        print \"one portion\", portion\n",
    "        to_keep = range(len(data_set))\n",
    "    else:                       \n",
    "        #to test code we select sample\n",
    "        print \"portion\", portion\n",
    "        random.seed(22)\n",
    "        to_keep = random.sample(range(len(data_set)), k=int(round(len(data_set)*portion)))            \n",
    "    \n",
    "    data_set = [data_set[i] for i in to_keep]\n",
    "    \n",
    "    perm = [i for i in to_keep]\n",
    "    \n",
    "    \n",
    "    valid_ids=set()\n",
    "    for element in data_set:\n",
    "        valid_ids.add(element[0])\n",
    "        valid_ids.add(element[1])\n",
    "        \n",
    "    if node_info is None:\n",
    "        tmp=[element for element in node_info if element[0] in valid_ids ]\n",
    "        node_info=tmp\n",
    "        del tmp\n",
    "\n",
    "\n",
    "\n",
    "    IDs = []\n",
    "    ID_pos={}\n",
    "    for element in node_info:\n",
    "        ID_pos[element[0]]=len(IDs)\n",
    "        IDs.append(element[0])\n",
    "        \n",
    "    print \"build graph\"\n",
    "    if graph is None:\n",
    "        graph = nx.Graph()\n",
    "        edges = []    \n",
    "        nodes = set()\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            nodes.add(source)\n",
    "            nodes.add(target)\n",
    "        \n",
    "            if with_labels:\n",
    "                if data_set[i][2] == \"1\":\n",
    "                    edges.append((source, target))\n",
    "            else:\n",
    "                edges.append((source, target))\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "        graph.add_nodes_from(nodes)\n",
    "        graph.add_edges_from(edges)\n",
    "    else:\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "    \n",
    "    clustering_coeff = nx.clustering(graph)\n",
    "    pr = nx.pagerank(graph, alpha=0.9)\n",
    "    jaccard_coeff = nx.jaccard_coefficient(graph, all_edges)\n",
    "    adamic_index = nx.adamic_adar_index(graph, all_edges)\n",
    "    pref_attach = nx.preferential_attachment(graph, all_edges)\n",
    "    ress_alloc = nx.resource_allocation_index(graph, all_edges)\n",
    "#     soundarajan = nx.cn_soundarajan_hopcroft(graph, all_edges)\n",
    "#     community = nx.within_inter_cluster(graph, all_edges)\n",
    "    hubs,auths=nx.hits(graph,max_iter=1000)\n",
    "\n",
    "    # graph features\n",
    "    neighbor_count_source = []\n",
    "    neighbor_count_target = []\n",
    "    neighbor_sim = []\n",
    "    clustering_coeff_source = []\n",
    "    clustering_coeff_target = []\n",
    "    graph_jaccoby_authors = []\n",
    "    graph_jaccard_coeff = []\n",
    "    pr_coeff_source = []\n",
    "    pr_coeff_target = []\n",
    "    \n",
    "    graph_ress_alloc = [p for u,v,p in ress_alloc]\n",
    "#     graph_soundarajan = [p for u,v,p in soundarajan]\n",
    "    \n",
    "    graph_adamic_index = [p for u,v,p in adamic_index]\n",
    "    graph_pref_attach = [p for u,v,p in pref_attach]\n",
    "    hubs_feature_source = []\n",
    "    hubs_feature_target = []\n",
    "    auths_feature_source = []\n",
    "    auths_feature_target = []\n",
    "    \n",
    "    graph_jaccard_coeff = [p for u,v,p in jaccard_coeff]\n",
    "    \n",
    "#     print np.size(graph_adamic_index)\n",
    "\n",
    "    \n",
    "    \n",
    "#     graph_cosine_authors = []\n",
    "\n",
    "    # we will use three basic features:\n",
    "\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    # tfidf similarities\n",
    "    tfidf_cos = []\n",
    "    \n",
    "#     topic model\n",
    "    topic_sim = []   \n",
    "    \n",
    "    # Important Topics\n",
    "#     U, S, V = np.linalg.svd(tfidf_mat.toarray())\n",
    "#     num_topics = 20\n",
    "#     Imp_topics = np.dot(A,V[:k,:].transpose())\n",
    "    \n",
    "    # Jaccard index\n",
    "    \n",
    "    # Cosine similarity\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     if tfidf_mat == None or topic_mat == None:\n",
    "#         corpus = [preprocess_string(element[5]) for element in node_info]\n",
    "        \n",
    "#         if tfidf_mat == None:\n",
    "#             vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "#             TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "#         else:\n",
    "#             TFIDF_matrix = tfidf_mat\n",
    "        \n",
    "#         if topic_mat == None:\n",
    "#             tm_vectorizer = text.CountVectorizer(stop_words='english', min_df=20)\n",
    "#             dtm = tm_vectorizer.fit_transform(corpus).toarray()\n",
    "    \n",
    "#             num_topics = 30\n",
    "#             clf = decomposition.NMF(n_components=num_topics)\n",
    "#             TOPIC_matrix = clf.fit_transform(dtm)\n",
    "#             TOPIC_matrix = TOPIC_matrix / np.sum(TOPIC_matrix, axis=1, keepdims=True)\n",
    "#         else:\n",
    "#             TOPIC_matrix = topic_mat                                    \n",
    "#     else:\n",
    "#         TFIDF_matrix = tfidf_mat\n",
    "#         TOPIC_matrix = topic_mat\n",
    "\n",
    "    counter = 0\n",
    "    for i in xrange(len(data_set)):\n",
    "        source = data_set[i][0]\n",
    "        target = data_set[i][1]\n",
    "    \n",
    "    \n",
    "        source_info = node_info[ID_pos[source]]\n",
    "        target_info = node_info[ID_pos[target]]\n",
    "    \n",
    "        # convert to lowercase and tokenize\n",
    "        source_title = source_info[2].lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        source_title = [token for token in source_title if token not in stpwds]\n",
    "        source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "        target_title = target_info[2].lower().split(\" \")\n",
    "        target_title = [token for token in target_title if token not in stpwds]\n",
    "        target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "        \n",
    "        \n",
    "        v1 = tfidf_mat[ID_pos[source],:].toarray()[0]\n",
    "        v2 = tfidf_mat[ID_pos[target],:].toarray()[0]        \n",
    "\n",
    "#         v1 = Imp_topics[ID_pos[source],:].toarray()[0]\n",
    "#         v2 = Imp_topics[ID_pos[target],:].toarray()[0]        \n",
    "\n",
    "        \n",
    "        temp_cosine = 0.0\n",
    "        \n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            temp_cosine = cosine(v1, v2)\n",
    "            \n",
    "        tfidf_cos.append(temp_cosine)\n",
    "        \n",
    "        \n",
    "        v1 = topic_mat[ID_pos[source],:]\n",
    "        v2 = topic_mat[ID_pos[target],:]\n",
    "        \n",
    "        topic_cosine = 0.0\n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            topic_cosine = cosine(v1, v2)         \n",
    "            \n",
    "        topic_sim.append(topic_cosine)\n",
    "            \n",
    "            \n",
    "        neighbors_source = len(list(graph.neighbors(source)))\n",
    "        neighbors_target = len(list(graph.neighbors(target)))\n",
    "        neighbor_count_source.append(neighbors_source)\n",
    "        neighbor_count_target.append(neighbors_target)\n",
    "        neighbors_source = set(list(graph.neighbors(source)))\n",
    "        neighbors_target = set(list(graph.neighbors(target)))\n",
    "        neighbor_sim.append(\n",
    "            len(neighbors_source.intersection(neighbors_target)))\n",
    "        clustering_coeff_source.append(clustering_coeff[source])\n",
    "        clustering_coeff_target.append(clustering_coeff[target])\n",
    "        authors_source = set(source_auth)\n",
    "        authors_target = set(target_auth)\n",
    "        graph_jaccoby_authors.append(float(len(authors_source.intersection(authors_target))) / float(len(authors_source.union(authors_target))))\n",
    "        pr_coeff_source.append(pr[source])\n",
    "        pr_coeff_target.append(pr[target])\n",
    "                \n",
    "        hubs_feature_source.append(hubs[source])\n",
    "        hubs_feature_target.append(hubs[target])\n",
    "        auths_feature_source.append(auths[source])\n",
    "        auths_feature_target.append(auths[target])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        \n",
    "        \n",
    "    \n",
    "        if counter % 10000 == 0:\n",
    "            print counter, \"training examples processsed\"\n",
    "        counter += 1\n",
    "        \n",
    "#     tfidf_source = vectorizer.fit(source_abstracts)._tfidf.idf_\n",
    "#     tfidf_target = vectorizer.fit(target_abstracts)._tfidf.idf_\n",
    "    \n",
    "\t\t\n",
    "    # convert list of lists into array\n",
    "    # documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "#     features = [overlap_title, temp_diff, comm_auth]\n",
    "#     if tfidf_mat is not None:\n",
    "#         features.append(tfidf_cos)\n",
    "#     if topic_mat is not None:\n",
    "#         features.append(topic_sim)\n",
    "        \n",
    "#     features = (np.array(features)).astype(float)\n",
    "    features = (np.array([overlap_title, temp_diff,\n",
    "                          comm_auth, tfidf_cos,\n",
    "                          topic_sim,\n",
    "                          neighbor_count_source,\n",
    "                          neighbor_count_target,\n",
    "                          neighbor_sim,\n",
    "                          clustering_coeff_source,\n",
    "                          clustering_coeff_target,\n",
    "                          graph_jaccoby_authors,\n",
    "                          pr_coeff_source,\n",
    "                          pr_coeff_target,\n",
    "                          graph_jaccard_coeff,\n",
    "                          graph_adamic_index,\n",
    "                          graph_pref_attach,\n",
    "                          graph_ress_alloc,\n",
    "                          hubs_feature_source,\n",
    "                          hubs_feature_target,\n",
    "                          auths_feature_source,\n",
    "                          auths_feature_target\n",
    "#                           graph_soundarajan\n",
    "                         ]).T).astype(float)\n",
    "    \n",
    "    \n",
    "#     print features\n",
    "\n",
    "    # scale\n",
    "    features = preprocessing.scale(features)    \n",
    "    \n",
    "    if with_labels:\n",
    "\n",
    "        # convert labels into integers then into column array\n",
    "        labels = [int(element[2]) for element in data_set]\n",
    "        labels = list(labels)\n",
    "        labels_array = np.array(labels)\n",
    "        \n",
    "        return perm, data_set, features, labels_array, graph\n",
    "    else:\n",
    "        labels_array = np.zeros(1)\n",
    "        return perm, data_set, features, labels_array, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n",
      "210000 training examples processsed\n",
      "220000 training examples processsed\n",
      "230000 training examples processsed\n",
      "240000 training examples processsed\n",
      "250000 training examples processsed\n",
      "260000 training examples processsed\n",
      "270000 training examples processsed\n",
      "280000 training examples processsed\n",
      "290000 training examples processsed\n",
      "300000 training examples processsed\n",
      "310000 training examples processsed\n",
      "320000 training examples processsed\n",
      "330000 training examples processsed\n",
      "340000 training examples processsed\n",
      "350000 training examples processsed\n",
      "360000 training examples processsed\n",
      "370000 training examples processsed\n",
      "380000 training examples processsed\n",
      "390000 training examples processsed\n",
      "400000 training examples processsed\n",
      "410000 training examples processsed\n",
      "420000 training examples processsed\n",
      "430000 training examples processsed\n",
      "440000 training examples processsed\n",
      "450000 training examples processsed\n",
      "460000 training examples processsed\n",
      "470000 training examples processsed\n",
      "480000 training examples processsed\n",
      "490000 training examples processsed\n",
      "500000 training examples processsed\n",
      "510000 training examples processsed\n",
      "520000 training examples processsed\n",
      "530000 training examples processsed\n",
      "540000 training examples processsed\n",
      "550000 training examples processsed\n",
      "560000 training examples processsed\n",
      "570000 training examples processsed\n",
      "580000 training examples processsed\n",
      "590000 training examples processsed\n",
      "600000 training examples processsed\n",
      "610000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, training_dataset, training_features, training_labels, graph = get_features(\"../input/training_set.txt\",\n",
    "                                                                    \"node_information.csv\",\n",
    "                                                                    1.0,\n",
    "                                                                    with_labels=True,\n",
    "                                                                    node_info = node_info,\n",
    "                                                                    tfidf_mat=TFIDF_matrix,\n",
    "                                                                    topic_mat=TOPIC_matrix)\n",
    "                                                                         #,topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_tf = pd.DataFrame(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_training_features = pd_tf.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079281</td>\n",
       "      <td>0.133143</td>\n",
       "      <td>-0.457673</td>\n",
       "      <td>-0.452204</td>\n",
       "      <td>0.050396</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.261447</td>\n",
       "      <td>0.052283</td>\n",
       "      <td>-0.019270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040206</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.406052</td>\n",
       "      <td>0.268435</td>\n",
       "      <td>0.025744</td>\n",
       "      <td>0.271548</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.014047</td>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.014047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.115546</td>\n",
       "      <td>-0.154287</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>0.109655</td>\n",
       "      <td>0.079764</td>\n",
       "      <td>0.084261</td>\n",
       "      <td>-0.228606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.123229</td>\n",
       "      <td>0.083171</td>\n",
       "      <td>0.084254</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>0.082107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.133143</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.242068</td>\n",
       "      <td>-0.187777</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>0.090475</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006639</td>\n",
       "      <td>-0.021780</td>\n",
       "      <td>0.246701</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>0.138354</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.017661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.457673</td>\n",
       "      <td>-0.115546</td>\n",
       "      <td>-0.242068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820837</td>\n",
       "      <td>-0.068157</td>\n",
       "      <td>-0.050779</td>\n",
       "      <td>-0.342215</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>0.049767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050944</td>\n",
       "      <td>-0.046655</td>\n",
       "      <td>-0.559050</td>\n",
       "      <td>-0.355364</td>\n",
       "      <td>-0.048515</td>\n",
       "      <td>-0.374230</td>\n",
       "      <td>-0.066237</td>\n",
       "      <td>-0.049163</td>\n",
       "      <td>-0.066237</td>\n",
       "      <td>-0.049163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.452204</td>\n",
       "      <td>-0.154287</td>\n",
       "      <td>-0.187777</td>\n",
       "      <td>0.820837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.086328</td>\n",
       "      <td>-0.069973</td>\n",
       "      <td>-0.378876</td>\n",
       "      <td>-0.025152</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>-0.553052</td>\n",
       "      <td>-0.388051</td>\n",
       "      <td>-0.059684</td>\n",
       "      <td>-0.391868</td>\n",
       "      <td>-0.076635</td>\n",
       "      <td>-0.060039</td>\n",
       "      <td>-0.076635</td>\n",
       "      <td>-0.060039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050396</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>-0.068157</td>\n",
       "      <td>-0.086328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082565</td>\n",
       "      <td>0.401773</td>\n",
       "      <td>-0.221971</td>\n",
       "      <td>-0.066994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970730</td>\n",
       "      <td>0.068612</td>\n",
       "      <td>0.082151</td>\n",
       "      <td>0.380527</td>\n",
       "      <td>0.441495</td>\n",
       "      <td>0.292207</td>\n",
       "      <td>0.901346</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.901346</td>\n",
       "      <td>0.096296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.109655</td>\n",
       "      <td>-0.022009</td>\n",
       "      <td>-0.050779</td>\n",
       "      <td>-0.069973</td>\n",
       "      <td>0.082565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334856</td>\n",
       "      <td>-0.012857</td>\n",
       "      <td>-0.302175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048097</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>-0.048428</td>\n",
       "      <td>0.317785</td>\n",
       "      <td>0.562982</td>\n",
       "      <td>0.250733</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.960460</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.960460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261447</td>\n",
       "      <td>0.079764</td>\n",
       "      <td>0.090475</td>\n",
       "      <td>-0.342215</td>\n",
       "      <td>-0.378876</td>\n",
       "      <td>0.401773</td>\n",
       "      <td>0.334856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038646</td>\n",
       "      <td>-0.146875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339386</td>\n",
       "      <td>0.313383</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.991610</td>\n",
       "      <td>0.571797</td>\n",
       "      <td>0.872551</td>\n",
       "      <td>0.388101</td>\n",
       "      <td>0.319114</td>\n",
       "      <td>0.388101</td>\n",
       "      <td>0.319114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.052283</td>\n",
       "      <td>0.084261</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>-0.025152</td>\n",
       "      <td>-0.221971</td>\n",
       "      <td>-0.012857</td>\n",
       "      <td>-0.038646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247545</td>\n",
       "      <td>-0.010589</td>\n",
       "      <td>0.150112</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>-0.100872</td>\n",
       "      <td>-0.007883</td>\n",
       "      <td>-0.195839</td>\n",
       "      <td>-0.018863</td>\n",
       "      <td>-0.195839</td>\n",
       "      <td>-0.018863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.019270</td>\n",
       "      <td>-0.228606</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.049767</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>-0.066994</td>\n",
       "      <td>-0.302175</td>\n",
       "      <td>-0.146875</td>\n",
       "      <td>0.081121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047717</td>\n",
       "      <td>-0.306877</td>\n",
       "      <td>0.112183</td>\n",
       "      <td>-0.136531</td>\n",
       "      <td>-0.183243</td>\n",
       "      <td>-0.104195</td>\n",
       "      <td>-0.100167</td>\n",
       "      <td>-0.276638</td>\n",
       "      <td>-0.100167</td>\n",
       "      <td>-0.276638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.095810</td>\n",
       "      <td>-0.002031</td>\n",
       "      <td>0.808051</td>\n",
       "      <td>-0.179715</td>\n",
       "      <td>-0.135523</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>0.066609</td>\n",
       "      <td>0.015243</td>\n",
       "      <td>0.016521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>0.177985</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.097737</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.040206</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.006639</td>\n",
       "      <td>-0.050944</td>\n",
       "      <td>-0.069291</td>\n",
       "      <td>0.970730</td>\n",
       "      <td>0.048097</td>\n",
       "      <td>0.339386</td>\n",
       "      <td>-0.247545</td>\n",
       "      <td>-0.047717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.059311</td>\n",
       "      <td>0.329750</td>\n",
       "      <td>0.416604</td>\n",
       "      <td>0.279748</td>\n",
       "      <td>0.830649</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>0.830649</td>\n",
       "      <td>0.057484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.123229</td>\n",
       "      <td>-0.021780</td>\n",
       "      <td>-0.046655</td>\n",
       "      <td>-0.067453</td>\n",
       "      <td>0.068612</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.313383</td>\n",
       "      <td>-0.010589</td>\n",
       "      <td>-0.306877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056631</td>\n",
       "      <td>0.301426</td>\n",
       "      <td>0.554511</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.934494</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.934494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.406052</td>\n",
       "      <td>0.083171</td>\n",
       "      <td>0.246701</td>\n",
       "      <td>-0.559050</td>\n",
       "      <td>-0.553052</td>\n",
       "      <td>0.082151</td>\n",
       "      <td>-0.048428</td>\n",
       "      <td>0.569894</td>\n",
       "      <td>0.150112</td>\n",
       "      <td>0.112183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059311</td>\n",
       "      <td>-0.056631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.591766</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>-0.047281</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>-0.047281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.268435</td>\n",
       "      <td>0.084254</td>\n",
       "      <td>0.103146</td>\n",
       "      <td>-0.355364</td>\n",
       "      <td>-0.388051</td>\n",
       "      <td>0.380527</td>\n",
       "      <td>0.317785</td>\n",
       "      <td>0.991610</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>-0.136531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329750</td>\n",
       "      <td>0.301426</td>\n",
       "      <td>0.591766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572162</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.351413</td>\n",
       "      <td>0.295489</td>\n",
       "      <td>0.351413</td>\n",
       "      <td>0.295489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.025744</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>-0.048515</td>\n",
       "      <td>-0.059684</td>\n",
       "      <td>0.441495</td>\n",
       "      <td>0.562982</td>\n",
       "      <td>0.571797</td>\n",
       "      <td>-0.100872</td>\n",
       "      <td>-0.183243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416604</td>\n",
       "      <td>0.554511</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.572162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525198</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.549024</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.549024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.271548</td>\n",
       "      <td>0.095207</td>\n",
       "      <td>0.138354</td>\n",
       "      <td>-0.374230</td>\n",
       "      <td>-0.391868</td>\n",
       "      <td>0.292207</td>\n",
       "      <td>0.250733</td>\n",
       "      <td>0.872551</td>\n",
       "      <td>-0.007883</td>\n",
       "      <td>-0.104195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279748</td>\n",
       "      <td>0.250317</td>\n",
       "      <td>0.606944</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.525198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235159</td>\n",
       "      <td>0.216636</td>\n",
       "      <td>0.235159</td>\n",
       "      <td>0.216636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.066237</td>\n",
       "      <td>-0.076635</td>\n",
       "      <td>0.901346</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.388101</td>\n",
       "      <td>-0.195839</td>\n",
       "      <td>-0.100167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830649</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>0.351413</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.235159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.014047</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>-0.049163</td>\n",
       "      <td>-0.060039</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.960460</td>\n",
       "      <td>0.319114</td>\n",
       "      <td>-0.018863</td>\n",
       "      <td>-0.276638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>0.934494</td>\n",
       "      <td>-0.047281</td>\n",
       "      <td>0.295489</td>\n",
       "      <td>0.549024</td>\n",
       "      <td>0.216636</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.035506</td>\n",
       "      <td>0.066083</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.066237</td>\n",
       "      <td>-0.076635</td>\n",
       "      <td>0.901346</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.388101</td>\n",
       "      <td>-0.195839</td>\n",
       "      <td>-0.100167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830649</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>0.351413</td>\n",
       "      <td>0.437698</td>\n",
       "      <td>0.235159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.014047</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>-0.017661</td>\n",
       "      <td>-0.049163</td>\n",
       "      <td>-0.060039</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.960460</td>\n",
       "      <td>0.319114</td>\n",
       "      <td>-0.018863</td>\n",
       "      <td>-0.276638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>0.934494</td>\n",
       "      <td>-0.047281</td>\n",
       "      <td>0.295489</td>\n",
       "      <td>0.549024</td>\n",
       "      <td>0.216636</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.079281  0.133143 -0.457673 -0.452204  0.050396  0.028234   \n",
       "1   0.079281  1.000000  0.007704 -0.115546 -0.154287  0.048388  0.109655   \n",
       "2   0.133143  0.007704  1.000000 -0.242068 -0.187777  0.003557 -0.022009   \n",
       "3  -0.457673 -0.115546 -0.242068  1.000000  0.820837 -0.068157 -0.050779   \n",
       "4  -0.452204 -0.154287 -0.187777  0.820837  1.000000 -0.086328 -0.069973   \n",
       "5   0.050396  0.048388  0.003557 -0.068157 -0.086328  1.000000  0.082565   \n",
       "6   0.028234  0.109655 -0.022009 -0.050779 -0.069973  0.082565  1.000000   \n",
       "7   0.261447  0.079764  0.090475 -0.342215 -0.378876  0.401773  0.334856   \n",
       "8   0.052283  0.084261  0.018753 -0.041077 -0.025152 -0.221971 -0.012857   \n",
       "9  -0.019270 -0.228606  0.022534  0.049767  0.087545 -0.066994 -0.302175   \n",
       "10  0.095810 -0.002031  0.808051 -0.179715 -0.135523  0.006442 -0.005125   \n",
       "11  0.040206  0.021183  0.006639 -0.050944 -0.069291  0.970730  0.048097   \n",
       "12  0.029164  0.123229 -0.021780 -0.046655 -0.067453  0.068612  0.993667   \n",
       "13  0.406052  0.083171  0.246701 -0.559050 -0.553052  0.082151 -0.048428   \n",
       "14  0.268435  0.084254  0.103146 -0.355364 -0.388051  0.380527  0.317785   \n",
       "15  0.025744  0.059600 -0.003734 -0.048515 -0.059684  0.441495  0.562982   \n",
       "16  0.271548  0.095207  0.138354 -0.374230 -0.391868  0.292207  0.250733   \n",
       "17  0.035506  0.066083 -0.000072 -0.066237 -0.076635  0.901346  0.159519   \n",
       "18  0.014047  0.082107 -0.017661 -0.049163 -0.060039  0.096296  0.960460   \n",
       "19  0.035506  0.066083 -0.000072 -0.066237 -0.076635  0.901346  0.159519   \n",
       "20  0.014047  0.082107 -0.017661 -0.049163 -0.060039  0.096296  0.960460   \n",
       "\n",
       "          7         8         9     ...           11        12        13  \\\n",
       "0   0.261447  0.052283 -0.019270    ...     0.040206  0.029164  0.406052   \n",
       "1   0.079764  0.084261 -0.228606    ...     0.021183  0.123229  0.083171   \n",
       "2   0.090475  0.018753  0.022534    ...     0.006639 -0.021780  0.246701   \n",
       "3  -0.342215 -0.041077  0.049767    ...    -0.050944 -0.046655 -0.559050   \n",
       "4  -0.378876 -0.025152  0.087545    ...    -0.069291 -0.067453 -0.553052   \n",
       "5   0.401773 -0.221971 -0.066994    ...     0.970730  0.068612  0.082151   \n",
       "6   0.334856 -0.012857 -0.302175    ...     0.048097  0.993667 -0.048428   \n",
       "7   1.000000 -0.038646 -0.146875    ...     0.339386  0.313383  0.569894   \n",
       "8  -0.038646  1.000000  0.081121    ...    -0.247545 -0.010589  0.150112   \n",
       "9  -0.146875  0.081121  1.000000    ...    -0.047717 -0.306877  0.112183   \n",
       "10  0.066609  0.015243  0.016521    ...     0.009849 -0.006269  0.177985   \n",
       "11  0.339386 -0.247545 -0.047717    ...     1.000000  0.041614  0.059311   \n",
       "12  0.313383 -0.010589 -0.306877    ...     0.041614  1.000000 -0.056631   \n",
       "13  0.569894  0.150112  0.112183    ...     0.059311 -0.056631  1.000000   \n",
       "14  0.991610 -0.030442 -0.136531    ...     0.329750  0.301426  0.591766   \n",
       "15  0.571797 -0.100872 -0.183243    ...     0.416604  0.554511  0.010421   \n",
       "16  0.872551 -0.007883 -0.104195    ...     0.279748  0.250317  0.606944   \n",
       "17  0.388101 -0.195839 -0.100167    ...     0.830649  0.138790  0.056062   \n",
       "18  0.319114 -0.018863 -0.276638    ...     0.057484  0.934494 -0.047281   \n",
       "19  0.388101 -0.195839 -0.100167    ...     0.830649  0.138790  0.056062   \n",
       "20  0.319114 -0.018863 -0.276638    ...     0.057484  0.934494 -0.047281   \n",
       "\n",
       "          14        15        16        17        18        19        20  \n",
       "0   0.268435  0.025744  0.271548  0.035506  0.014047  0.035506  0.014047  \n",
       "1   0.084254  0.059600  0.095207  0.066083  0.082107  0.066083  0.082107  \n",
       "2   0.103146 -0.003734  0.138354 -0.000072 -0.017661 -0.000072 -0.017661  \n",
       "3  -0.355364 -0.048515 -0.374230 -0.066237 -0.049163 -0.066237 -0.049163  \n",
       "4  -0.388051 -0.059684 -0.391868 -0.076635 -0.060039 -0.076635 -0.060039  \n",
       "5   0.380527  0.441495  0.292207  0.901346  0.096296  0.901346  0.096296  \n",
       "6   0.317785  0.562982  0.250733  0.159519  0.960460  0.159519  0.960460  \n",
       "7   0.991610  0.571797  0.872551  0.388101  0.319114  0.388101  0.319114  \n",
       "8  -0.030442 -0.100872 -0.007883 -0.195839 -0.018863 -0.195839 -0.018863  \n",
       "9  -0.136531 -0.183243 -0.104195 -0.100167 -0.276638 -0.100167 -0.276638  \n",
       "10  0.074000  0.010471  0.097737  0.010702  0.002533  0.010702  0.002533  \n",
       "11  0.329750  0.416604  0.279748  0.830649  0.057484  0.830649  0.057484  \n",
       "12  0.301426  0.554511  0.250317  0.138790  0.934494  0.138790  0.934494  \n",
       "13  0.591766  0.010421  0.606944  0.056062 -0.047281  0.056062 -0.047281  \n",
       "14  1.000000  0.572162  0.926908  0.351413  0.295489  0.351413  0.295489  \n",
       "15  0.572162  1.000000  0.525198  0.437698  0.549024  0.437698  0.549024  \n",
       "16  0.926908  0.525198  1.000000  0.235159  0.216636  0.235159  0.216636  \n",
       "17  0.351413  0.437698  0.235159  1.000000  0.200222  1.000000  0.200222  \n",
       "18  0.295489  0.549024  0.216636  0.200222  1.000000  0.200222  1.000000  \n",
       "19  0.351413  0.437698  0.235159  1.000000  0.200222  1.000000  0.200222  \n",
       "20  0.295489  0.549024  0.216636  0.200222  1.000000  0.200222  1.000000  \n",
       "\n",
       "[21 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12292b490>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAHVCAYAAACXAw0nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12256f250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr_training_features, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr_training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, test_dataset, test_features, _, _= get_features(\"../input/testing_set.txt\", \"node_information.csv\", 1.0,\n",
    "                                                  with_labels=False,\n",
    "                                                  node_info = node_info,\n",
    "                                                  tfidf_mat=TFIDF_matrix,\n",
    "                                                  topic_mat=TOPIC_matrix,\n",
    "                                                  graph = graph)\n",
    "#                                                   topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=300)\n",
    "classifier.fit(training_features, training_labels)\n",
    "pred=classifier.predict(test_features)\n",
    "results = np.concatenate((np.reshape(IDs,(np.size(pred), 1)), np.reshape(pred, (np.size(pred),1))), axis=1)\n",
    "with open(\"new_submission.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, )\n",
    "    writer.writerow(['id','prediction'])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
